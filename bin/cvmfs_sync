#!/usr/bin/env python3

import os
import stat
import sys
import time
import errno
import xattr
import queue
import random
import signal
import shutil
import logging
import fnmatch
import optparse
import urllib.parse
import threading
import subprocess
import multiprocessing

try:
    import subprocess32
except ImportError:
    subprocess32 = None

oldpath = sys.path
sys.path = sys.path[1:]

import XRootD.client

sys.path = oldpath

g_failed_files = []
g_processed_files = []
g_bytes_xfer = 0
g_skip_count = 0
g_failed_dirs = []
g_failure_reasons = []

def full_norm_path(path):
    path = os.path.normpath(path)
    if path.startswith("//"):
        path = path[1:]
    return path


def parse_opts():
    parser = optparse.OptionParser(usage="%prog [options] src1 [src2 ...] dest")
    parser.add_option("-n", "--concurrency", dest="concurrency",
                       help="Number of helper processes for processing data", default=4,
                       type="int")
    parser.add_option("-m", "--metadata-concurrency", dest="metadata_concurrency",
                       help="Number of helper threads for processing metadata", default=2,
                       type="int")
    parser.add_option("-f", "--failed", dest="failed",
                      help="Output filename for list of failed files")
    parser.add_option("-i", "--ignore", dest="ignore",
                      help="Unix glob of filenames to ignore.")
    parser.add_option("--include", dest="include",
                      help="Unix glob of paths to include.")
    parser.add_option("-v", "--verbose", dest="verbose", default=False, action="store_true",
                      help="Enable verbose printout.")
    parser.add_option("-t", "--max-time", dest="max_time", type="int", default=0,
                      help="Time threshold for starting new transfers, in minutes.")

    opts, args = parser.parse_args()
    if len(args) < 2:
        print("Need at least one source and destination", file=sys.stderr)
        parser.print_usage()
        sys.exit(1)

    if opts.ignore:
        opts.ignore = [full_norm_path(i) for i in opts.ignore.split(",")]
    else:
        opts.ignore = []
    if opts.include:
        opts.include = [full_norm_path(i) for i in opts.include.split(",")]
    else:
        opts.include = []

    if opts.verbose:
        logging.basicConfig(level=logging.DEBUG, format="%(levelname)-8s: %(message)s")
    else:
        logging.basicConfig(level=logging.INFO, format="%(levelname)-8s: %(message)s")

    return opts, args


def should_ignore_path(entry_name, ignore, include):
    norm_entry_name = full_norm_path(entry_name)
    should_ignore = bool(include)
    path_count = len(norm_entry_name.split("/"))
    for include_rule in include:
        if include_rule.startswith("/"):
            include_rule = "/".join(include_rule.split("/")[:path_count])
        if fnmatch.fnmatch(norm_entry_name, include_rule):
            should_ignore = False
            break
    if should_ignore:
        return True
    for ignore_rule in ignore:
         if fnmatch.fnmatch(norm_entry_name, ignore_rule):
             return True
    return False


def should_ignore_filename(filename):
    #ignore file names with illegal characters ?, *, >, <, [, ], |, ;, &, (, ), $, \
    #https://xrootd.slac.stanford.edu/doc/dev52/XRdv400.htm#_Toc66820389
    should_ignore = False
    bad_chars = ['?', '*', '>', '<', '[', ']', '|', ';', '&', '(', ')', '$', '\\']
    for i in bad_chars:
        if i in filename:
            logging.debug("Ignoring file name %s" % filename)
            should_ignore = True
            break
    return should_ignore


def graft_filename(filename):
    parent_dir, fname = os.path.split(filename)
    return os.path.join(parent_dir, ".cvmfsgraft-" + fname)


def should_skip(input_url, output_filename, size, output_mode):
    global g_skip_count
    graftfile = graft_filename(output_filename)

    # CVMFS checksum format has trouble with very large files.
    # There's additionally suspicious-looking failures around zero-sized
    # files.
    if (size > 500*(1024**3)) or (size == 0):
        return True

    if os.path.exists(output_filename):
        st = os.stat(output_filename)

        # Compare size and mode to source
        size_ok = size == st.st_size
        mode_ok = output_mode == stat.S_IMODE(st.st_mode)

        if size_ok and mode_ok:
            g_skip_count += 1
            return True
        elif os.path.exists(graftfile):
            graft_size = -1
            with open(graftfile, "r") as fp:
                for line in fp:
                    if line.startswith("size="):
                        try:
                            graft_size = int(line.strip().split("=", 1)[-1])
                            break
                        except:
                            pass
            if size == graft_size and mode_ok:
                g_skip_count += 1
                return True
            logging.warning("Graft size/mode mismatch!  Regenerating file %s (size %d) -> %s (graft size %d)" % \
                    (input_url, size, output_filename, graft_size))
        else:
            logging.warning("Size/mode mismatch!  Regenerating file %s (size %d) -> %s (size %d)" % \
                    (input_url, size, output_filename, st.st_size))
            os.unlink(output_filename)
    try:
        os.makedirs(os.path.split(output_filename)[0])
    except OSError as oe:
        if oe.errno != errno.EEXIST:
            raise
    return False


def process_download_file(xrootd_url, output_filename, output_mode, deadline):
    """
    Download an entire file to the local host and stream its contents through cvmfs_swissknife
    in order to create checksums.
    """
    if (deadline > 0) and (time.time() > deadline):
        raise Exception("URL %s timed out when still in processing queue." % xrootd_url)

    logging.info("Grafting %s to %s" % (xrootd_url, output_filename))

    # Fork cvmfs_swissknife
    readfp, writefp = os.pipe()
    pid = os.fork()
    if not pid:
        try:
            os.dup2(readfp, 0)
            os.close(writefp)
            args = ['cvmfs_swissknife', 'graft', '-i', '-', '-o', output_filename, '-Z', 'none', '-c', '24']
            os.execvp("cvmfs_swissknife", args)
        finally:
            raise Exception("Failed to exec cvmfs_swissknife for processing URL %s" % xrootd_url)

    # Open remote file
    # After fork to reduce risk of xrootd thread cleanup deadlock
    fp = XRootD.client.File()
    status = fp.open(xrootd_url)[0]
    if status.status:
        # Cleanup cvmfs_swissknife
        os.kill(pid, signal.SIGKILL)
        os.waitpid(pid, 0)
        raise Exception("Failed to open file (%s) in Xrootd: %s" % (xrootd_url, status))

    # Check deadline again; file-open may have taken awhile for an unresponsive server.
    if (deadline > 0) and (time.time() > deadline):
        # Cleanup cvmfs_swissknife
        os.kill(pid, signal.SIGKILL)
        os.waitpid(pid, 0)
        raise Exception("URL %s timed out while being opened." % xrootd_url)

    os.close(readfp)
    next_offset = 0
    while True:
        if (deadline > 0) and (time.time() > deadline):
            os.kill(pid, signal.SIGKILL)
            os.waitpid(pid, 0)
            raise Exception("Timed out when transferring URL %s" % xrootd_url)

        status, response = fp.read(offset=next_offset, size=1024*1024)
        if status.status:
            os.kill(pid, signal.SIGKILL)
            os.waitpid(pid, 0)
            raise Exception("Failed to create read request for %s; status: %s; killing graft %d" % (xrootd_url, str(status), pid))

        # Empty response indicates EOF.
        if (response == None) or (len(response) == 0):
            logging.debug("Finished processing URL %s" % xrootd_url)
            os.close(writefp)
            os.waitpid(pid, 0)
            os.chmod(output_filename, output_mode)
            break
        else:
            try:
                os.write(writefp, response)
            except OSError as oe:
                if oe.errno != errno.EPIPE:
                    raise
                logging.error("cvmfs_swissknife process unexpectedly died.")
                return False
            next_offset += len(response)
    return next_offset


def process_checksum_file(gridftp_url, xrootd_url, output_filename, output_mode, deadline):
    """
    Process and create a CVMFS checksum / graft file.

    If a GridFTP server is available, use that to generate a checksum file.  Otherwise,
    utilize Xrootd to download the file to memory and checksum it locally.
    """
    if not gridftp_url:
        return process_download_file(xrootd_url, output_filename, output_mode, deadline)

    if (deadline > 0) and (time.time() > deadline):
        raise Exception("URL %s timed out when still in processing queue." % xrootd_url)

    parsed = urllib.parse.urlparse(gridftp_url)
    logging.info("Querying GridFTP server for %s" % gridftp_url)
    try:
        if subprocess32:
            output = subprocess32.check_output(["uberftp", parsed.netloc, "quote cksm cvmfs 0 -1 %s" % parsed.path], timeout=5*60)
        else:
            output = subprocess.check_output(["uberftp", parsed.netloc, "quote cksm cvmfs 0 -1 %s" % parsed.path])
    except subprocess.CalledProcessError as cpe:  # https://bugs.python.org/issue9400 - CalledProcessError messes up multiprocessing
        raise Exception("UberFTP call failed. (cmd=%s, output=%s, returncode=%s)" % (cpe.cmd, cpe.output, str(cpe.returncode)))
    except Exception as e:
        logging.error("Unhandled exception %s" % str(e))
        raise Exception("UberFTP call failed with an unhandled exception.")
    cvmfs_cksum = None
    for line in output.splitlines():
        line = line.strip()
        if line.startswith("500"):
            raise Exception(line)
        elif line.startswith("213 "):
            cvmfs_cksum = line[4:]
    if cvmfs_cksum:
        with open(output_filename, "w") as fp:
            pass
        graftfile = cvmfs_cksum.replace(";", "\n")
        with open(graft_filename(output_filename), "w") as fp:
            fp.write(graftfile + "\n")
    else:
        raise Exception("Remote GridFTP server did not return a CVMFS checksum")
    return 0


def process_files(base_url, gridftp_base_url, output_dir, count, ignore=[], include=[], thread_count=2, deadline=0):
    global g_bytes_xfer

    current = 0
    worklist = []

    xrootd_base_parsed = urllib.parse.urlparse(base_url)
    xrootd_connection_info = xrootd_base_parsed.scheme + "://" + xrootd_base_parsed.netloc
    xrootd_directory = xrootd_base_parsed.path

    # First, try to retrieve the file
    pool = multiprocessing.Pool(count)
    futures = []
    for filename, statinfo in process_dir(xrootd_connection_info, xrootd_directory, output_dir, ignore=ignore, include=include, thread_count=thread_count):
        if deadline and (time.time() > deadline):
            logging.error("Passed deadline when scanning directory tree")
            break
        if len(futures) >= 50000:
        #if len(futures) >= 200:
            logging.info("Queue of 50k files to checksum has been reached; will attempt remainder next time.")
            break
        if gridftp_base_url:
            gridftp_url = gridftp_base_url + filename
        else:
            gridftp_url = None
        xrootd_url = base_url + filename
        output_filename = output_dir + filename
        input_url = base_url + filename

        size = statinfo.size
        if (statinfo.flags & XRootD.client.flags.StatInfoFlags.X_BIT_SET):
            output_mode = 0o755 # Is executable, set 755
        else:
            output_mode = 0o644 # Else 644

        if should_skip(input_url, output_filename, size, output_mode):
            continue
        if should_ignore_filename(filename):
            continue
        future = pool.apply_async(process_checksum_file, (gridftp_url, xrootd_url, output_filename, output_mode, deadline))
        future.filename = filename
        future.size = size
        futures.append(future)
        #break
    pool.close()
    while futures:
        timed_out_futures = []
        for future in futures:
            try:
                future.get(300)
                g_processed_files.append(future.filename)
                g_bytes_xfer += future.size
            except multiprocessing.TimeoutError:
                logging.warning("Timed out when waiting for file %s; will retry later." % future.filename)
                #timed_out_futures.append(future)
            except Exception as e:
                logging.error("Failure when trying to checksum %s (size %d); will regenerate.  Exception type: %s; %s" % (future.filename, future.size, type(e).__name__, str(e)))
                if "permission denied" in str(e).lower():
                    print("Skipping permission denied error")
                else:
                    g_failed_files.append(gridftp_base_url + "/" + future.filename)
                #g_failed_files.append(gridftp_base_url + "/" + future.filename)
        futures = timed_out_futures
    logging.info("All jobs processed - closing up pool.")
    pool.join()


def lookup_dataserver(redirector, path, timeout=10):
    """
    Look up an individual data server from a redirector.
    If lookup fails, return the original redirector.

    For sites where the redirector does not have the cluster filesystem
    mounted, dirlist requests will fail. Use this function to select
    a data server.
    """

    fs = XRootD.client.FileSystem(redirector)

    handler = XRootD.client.utils.AsyncResponseHandler()
    status = fs.stat(path, timeout=timeout, callback=handler)
    status, _, hostlist = handler.wait()

    if status.ok:
        # Return the last URL in the hostlist (the data server)
        return(str(list(hostlist)[-1].url))
    else:
        return redirector


def list_dir(work_queue, results_queue, fs):
    """
    Helper thread for listing directories in parallel
    """
    while True:
        cwd = work_queue.get()
        logging.debug("%s working on directory listing of %s" % (threading.current_thread().name, cwd))
        starttime = time.time()
        status = None
        dirlist = None
        try:
            status, dirlist = fs.dirlist(cwd,
                                        flags=XRootD.client.flags.DirListFlags.STAT)
        except Exception as e:
            logging.error("Exception encoutered when listing directory %s: %s" % (cwd, str(e)))
        finally:
            results_queue.put((status, dirlist, cwd, starttime), True)
            work_queue.task_done()


def start_listing_workers(thread_count, work_queue, results_queue, fs):
    """
    Helper function for launching the directory spider threads
    """
    for count in range(thread_count):
        t = threading.Thread(target=list_dir, args=(work_queue, results_queue, fs), name="Directory list worker %d" % count)
        t.setDaemon(True)
        t.start()


def process_dir(base_url, directory, output_dir, ignore=[], include=[], thread_count=2):
    global g_failed_dirs
    base_data_url = lookup_dataserver(base_url, directory)
    fs = XRootD.client.FileSystem(base_data_url)
    logging.info("Switching from redirector %s to data server %s" % (base_url, base_data_url))
    logging.info("Starting processing of top-level directory %s", directory)
    worklist = [directory]
    base_len = len(directory)
    top_level_dir = directory

    # Create our input / output queues for helper threads
    work_queue = queue.Queue(maxsize=thread_count)
    work_sent = 0
    results_queue = queue.Queue() # shouldn't block
    results_received = 0
    start_listing_workers(thread_count, work_queue, results_queue, fs)

    while worklist or (work_sent != results_received):

        # Try to make sure there is sufficient work for the queues.
        while worklist and not work_queue.full():
            cwd = worklist.pop()

            # Ugly hack for OSG -- not clear how to clean this up.
            if include and include[0] == '/user/*/public/*':
                full_cwd = full_norm_path(cwd)
                pathcount = len(full_cwd.split("/"))
                if full_cwd.startswith("/user") and pathcount == 3:
                    logging.debug("OSG stash: skipping private directory %s" % full_cwd)
                    cwd += "/public"

            cwd = os.path.normpath(cwd)
            logging.debug("Queueing %s for processing" % cwd)
            work_sent += 1
            work_queue.put(cwd, block=True)

        # Process results serially.
        status, dirlist, cwd, starttime = results_queue.get(True)
        logging.info("Processing entries of %s" % cwd)
        results_queue.task_done()
        results_received += 1
        if status.status:
            logging.warning("Failed to list directory (%s) skipping: %s" % (cwd, status))
            if "permission denied" in str(status).lower():
                print("Skipping permission denied error")
            else:
                g_failed_dirs.append(str(cwd))
            continue

        xrootd_cwd_names = set()
        entries = list(dirlist.dirlist)
        logging.debug("Directory %s has %d entries" % (cwd, len(entries)))
        if cwd == top_level_dir:
            print(("%s is the top level directory" % (cwd)))
            if len(entries) == 0:
               logging.warning("The top level directory is empty, not removing any subdirectories.")
               g_failure_reasons.append("Top level directory empty")
               break
        random.shuffle(entries)
        for entry in entries:
            if should_ignore_path(cwd + "/" + entry.name, ignore, include):
                continue
            # Sometimes the remote XRootD will prefix the name with '/'
            xrootd_cwd_names.add(entry.name.split("/")[-1])
            if entry.statinfo.flags & XRootD.client.flags.StatInfoFlags.IS_DIR:
                worklist.append(cwd + "/" + entry.name)
            else:
                fname = cwd + "/" + entry.name
                yield (fname[base_len:], entry.statinfo)
        cwd_output = output_dir + "/" + cwd[len(directory):]
        try:
            cwd_output_names = set(os.listdir(cwd_output))
        except OSError as oe:
            if oe.errno == errno.ENOENT:
                logging.info("Destination directory (%s) does not exist; not checking for removal candidates." % cwd_output)
                continue
            else:
                raise
        remaining_names = cwd_output_names.difference(xrootd_cwd_names)
        for name in remaining_names:
            # Ignore presence of hidden files (such as the graft files).
            if name.startswith("."): continue
            if should_ignore_path(cwd + "/" + name, ignore, include):
                continue
            fname = os.path.join(cwd_output, name)
            logging.info("Candidate for removal: %s" % fname)
            try:
                os.unlink(fname)
            except OSError as oe:
                if oe.errno == errno.EISDIR:
                    logging.info("Removing entire subdirectory.")
                    shutil.rmtree(fname, True) # Remove directory tree, ignoring errors.
                else:
                    logging.warning("Failed to remove file %s: %s" % (fname, str(oe)))
        # I'd like to record the last visit for each subdirectory - but this triggers a lot
        # of churn in the catalogs, even when there's no actual changes.  Perhaps in the future,
        # we could just record changes.
        #xattr.setxattr(cwd_output, "user.last_visit", str(int(starttime)))

def main():
    opts, args = parse_opts()
    dest = args[-1]
    srcs = args[:-1]
    print("Inputs to process:", ",".join(srcs))
    concurrency = opts.concurrency
    starttime = time.time()

    if opts.max_time > 0:
        deadline = opts.max_time*60 + starttime
    else:
        deadline = starttime + 86400*7

    for src in srcs:
        if time.time() > deadline:
            break
        source_filelist = []
        info = src.split(",")
        gridftp_src = ''
        if len(info) == 2:
            src, gridftp_src = info
        process_files(src, gridftp_src, dest, concurrency, ignore=opts.ignore, include=opts.include, thread_count=opts.metadata_concurrency, deadline=deadline)

    processing_time = time.time() - starttime
    print("Total of %.1f GB in %d files processed in %.2f seconds." % (g_bytes_xfer/(1024.**3), len(g_processed_files), processing_time))
    print("%d file checksums were skipped because they already exist locally." % g_skip_count)
    # Processing rate is not sensible as we don't know if this is a "new" checksum.
    #print "Processing rate: %.2fMB/s" % (g_bytes_xfer/processing_time/(1024*1024.))

    if opts.failed:
        fh = open(opts.failed, "w")
        fh.write("\n".join(g_failed_files))
        fh.write("\n".join(g_failed_dirs))
    if g_failed_files:
        print("There were %d file failures." % len(g_failed_files))
        g_failure_reasons.append("File failures")
        for file in g_failed_files:
            print(file)
            # TODO: cleanup partial grafts
    if g_failed_dirs:
        print("There were %d listing failures." % len(g_failed_dirs))
        g_failure_reasons.append("Directory failures")
        for dir in g_failed_dirs:
            print(dir)
    if g_failure_reasons:
        print(g_failure_reasons)
        return 1
    else:
        print("Synchronization completed successfully!")
        return 0

if __name__ == '__main__':
    sys.exit(main())


